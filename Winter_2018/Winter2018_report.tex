\documentclass{report}
\title{Summary Report on Four Different Presentations}
\author{Abdullah-Al-Zubaer Imran}
\date{}

\renewcommand\thesection{\arabic{section}}

\begin{document}
\maketitle

\section{Say Hello to Waymo}
This talk was given by Dr. Khaled Refaat, a senior robotics researcher at Waymo. The presenter basically talked about their journer at Waymo with the self-driving car and shared interesting experiences throughout this journey. Waymo is one of the leading self-driving technology companies in the world. The mission of Waymo is to make it safe and easy for everyone to get around--without the need for having any real driver in the car. Waymo started their journey in 2009 as a part of Google named as Google's self-driving car. Later, in 2016 they became a separate company and took the name "Waymo." However, they are still under the umbrella of their parent company Alphabet. Although automobiles came in 1885, the dream of self-driving car happened just in 1950's. And the dream started becoming real when DARPA self-driving challenge initiated. In the first DARPA challenge, Carnegie Mellon University won in a race of 7.4 miles. Stanford won the second DARPA challenge. The third DARPA challenge was quite different than the first two challenges; this time it was urban challenge with light traffic. According to the road accident statistics, 1.2 million people die in the USA because of 94\% car accidents. And no surprise, most of those accidents are caused by human error. Therefore, with the goal that everyone could get around easily and safely, without the issue of tired, drunk or distracted driving. As the car handles all of the driving itself, the commuting time could be spent doing whatever someone wants.\\

Waymo started their self-driving car on freeways in 2009 with a Lexus car. They experienced that people on the roads did not pay attention. Later in 2012, they started in city streets where the obstacles include pedestrians. After designing their own car named Firefly in 2015, this experiment got more attention and in 2016 , the self-driving car company Waymo started. Since 2009, 3 million miles have been driven in real streets and 2.5 billion virtual miles in simulation. The performance measure in terms of disengagement has certainly improved to 0.20 in 2016 compared to 0.80 in 2015. The car is equipped with many sensors including vision system and LiDAR. The first thing Waymo car does is mapping the roads; this map although not availble for mass use, is much richer than the Google map. Then large number of pedestrain information is collected and machine learning techniques are used to deal with that. Based on the experiments through driving virtually and in real streets, Waymo's self-driving car is getting better and better. It can be said that the day is not too far the dream which was dreamt in the 50s is going to be real very soon.         

\newpage

\section{Computation and Vision}
This talk was given by Dr. Alex Berg who is an associate professor in computer science at UNC Chapel Hill. The presenter talked about the historical perspectives of the fundamental computer vision problems: the physics behind it and the computation associated with that. Vision basically a sensor to view the world around us with immense challenge and opportunities of computation. Human communication and reasoning in the complexities of physical structure are involved in the vision problems. The computations in vision problem include detection, recognition, and understanding the structures. Detection and recognition problems could be of detecting edges, some object or part of it, face detection or identification, category detection or recognition, scene recognition or understanding, etc. There have been several efforts in developing object detection frameworks. Among all the recent and advanced techniques, single shot detector is the fastest detection framework in comparison to YOLO and faster r-cnn models. \\  

However, the tasks associated to computation and visual recognition still remained constant over the history of computer vision. From 1970's ideas for statistical pattern theory and geometric hashing to 2010-2017's ImageNet challenge of object classification, the key computational problems haven't changed much. Only the approch to visual computation and recognition with the introduction of novel techniques has been getting better and better. For example, the classification error of the ImageNet challenge decreased over time with the application of more advanced techniques and algorithms, especially the introduction of deep neural network. But for the effective use of deep neural networks in the solving classification problems, it requires diverse large scale annotated data, powerful deep neural network algorithm, and fast computing GPUs. Deep learning can however be associated to visually descriptive language which depicts information about the visual world and guidance for visual recognition. Some natural language processing problems may include understanding and predicting importance in images, how should we refer to specific content, collective phrase fusion for natural image descriptions, object appearance, pose, etc. Moreover, along with datasets, algorithms, and computation, vision approaches can be integrated to use in everyday robotics.      

\newpage


\section{Big Data in Climate and Earth Sciences: Challenges and Opportunities for Machine Learning}
This talk was delivered by Professor Vipin Kumar, a professor at the University of Minnesota. In this talk, the presenter discussed various challenges in analyzing massive climate datasets and the opportunities for advancing machine learning. This advancement could also depict the progress of the science of climate change in the context of monitoring the state of the tropical forests and surface water on a global scale. The transformation of the climate and earth sciences has been advantageous with data-rich environment from data-poor environment. Because of the large number of satellites observing earth, massive earth and its environment is being generated. Moreover, physics-based earth system models running on large-scale computational platforms are also being utilized for generating earth data on larger-scale.\\

However, the big data problem in climate could pose some challenges where data could be of varying resolutions, different scales, larger-scale noise, uncertainty, and so forth. Not only that, the generated climate data has complex underlined physical properties while being generated. Understanding climate change as a data-driven approach has been ongoing in order to extract the latent properties, develop algorithms, patterns of interest, etc. The primary source being the satellite data provided by NASA where every pixel covers 16km of the entire world. This dataset is involved in big data in earth system monitoring. Monitoring global change work includes global mapping of forest fires, mapping of plantation dynamics in tropical forests, global mapping of inland water dynamics, etc. It is very challenging problem to develop a predictive model for the global mapping of forest fires and deploy a really working model. The challenge grows when the predictive modeling involves a target class using imperfect labels. Leanring with imperfect labels is the key to develop and deploy an efficient model for dynamic mapping the burned area on global-scale.   

\newpage

\section{AI and Computer Vision in Neurological Care}
This talk was delivered by Dr. Fabien Scalzo, an assistant professor of computer science, neurology, and electrical engineering at UCLA. In this talk, Dr. Fabien outlined the motivation and rationale behind the use of machine learning in neurology and described several computational models recently developed for providing enhanced diagnostic and decision support system. Like the other task in computer vision, machine learning has been demonstrating tremendous promises for solving medical imaging problems requiring rapid analysis of complex information. Machine learning-based decision support tools are being integrated in neurology with the availability of multimodal large-scale imaging datsets. Although the machine learning techniques are used in rapid scale, the goal is not to replace the doctors or neurosurgeons; rather to make them better. Machine learning gives computers the ability to learn to perform a task without being explicitly programmed especially, in case when there is no clue about the underlined function. \\

Among many other works in neuroscience, the use of TCD waveform for the diagnosis of brain injuries is noteworthy. At first, with the digitization of data from TCD, signal is captured. Then, feature extraction is performed by identifying 128 waveform metrics. And finally, using machine learning algorithm, classification was performed. Transition from logisitc regression and decision trees to convolutional neural network improved accuracy in classifying ICP waveforms between healthy and abnormal cases. Another application presented was on ischemic stroke-- blood clots stop blood flow to an area of the brain. A neurosurgeon either drills to that area and evacuates the pressure or injects a chemical to vessels. Perfusion angiography by taking X-ray shows overlap of vessels. Replacing manual navigation of the video with pattern recognition technique, different observed components are estimated and characterizing those components, an occlusion could be detected. Neuroimaging techniques also involve computational fluid mechanics and 3D prinitng of blood vessels. In upcoming days, opportunities of AI are going to be but not limited to advanced reasoning, trasparency, uncertainty in data, prior knowledge, and dealing with limited size datasets.      


\end{document}
